{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13489411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFDirectoryLoader(\"./reliability\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "final_documents = text_splitter.split_documents(documents)\n",
    "len(final_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163c0eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", \n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb5e8955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.45106328e-02 -2.08494905e-02  1.64920650e-03 -1.06336800e-02\n",
      "  3.61050479e-02  2.28325855e-02  1.16156891e-01  2.23064553e-02\n",
      "  3.57013084e-02  1.03948440e-03  5.20878136e-02 -4.45944406e-02\n",
      "  7.52096176e-02 -1.51128098e-02 -1.36651052e-02  5.11960611e-02\n",
      " -5.10529689e-02  4.88237515e-02  6.77977651e-02 -6.25571758e-02\n",
      "  1.05117142e-01 -2.29992904e-02 -1.09677268e-02 -1.78853739e-02\n",
      "  3.67015041e-02  6.49010912e-02 -3.66079248e-02  2.02322677e-02\n",
      " -1.02022775e-02 -1.25149742e-01 -3.49860527e-02  6.06301706e-03\n",
      " -6.28810450e-02 -7.15822428e-02 -1.33577716e-02 -2.73708608e-02\n",
      "  9.07621067e-03 -2.34484598e-02 -5.45840652e-04  5.71681699e-03\n",
      " -5.38177267e-02  8.66935682e-03 -1.90860834e-02 -5.22610359e-02\n",
      " -3.35520543e-02 -1.43058859e-02 -4.52715624e-03 -4.21434939e-02\n",
      " -9.98236518e-03  3.33234593e-02  2.27042176e-02  1.06239934e-02\n",
      "  6.61173612e-02  5.33741005e-02  6.42898399e-03  3.21992598e-02\n",
      "  8.95208046e-02  7.33826682e-02  3.03784572e-02  1.42466975e-04\n",
      "  4.36370298e-02 -2.62018461e-02 -1.44543409e-01  3.49621251e-02\n",
      "  9.34320241e-02 -1.53116919e-02 -2.51755752e-02 -2.97075100e-02\n",
      " -1.06299454e-02  4.49004956e-02 -4.99849878e-02 -1.14553217e-02\n",
      " -4.73455191e-02  1.12532042e-01  5.23797050e-03  1.46637135e-03\n",
      "  2.95525920e-02 -8.77757594e-02 -1.63516141e-02  3.81954350e-02\n",
      "  7.13536814e-02  1.57716870e-02 -4.60089557e-02  2.40247361e-02\n",
      " -6.41016737e-02 -5.83306476e-02  2.92512234e-02 -5.02566434e-03\n",
      "  1.94687899e-02  1.38217593e-02 -1.61459967e-02  8.33131000e-02\n",
      " -3.46381171e-03  2.56687943e-02 -1.03107151e-02 -1.10221123e-02\n",
      "  4.41226773e-02 -5.61930472e-03  7.40347244e-03  3.27550083e-01\n",
      " -3.00724879e-02 -1.22947311e-02  2.32549738e-02  5.95516898e-02\n",
      "  2.29938906e-02 -1.22457361e-02 -2.76216790e-02  3.46608460e-03\n",
      " -6.04905151e-02 -4.09382880e-02  2.67181434e-02 -4.70078848e-02\n",
      " -2.28418298e-02 -3.79912406e-02  4.32935357e-03 -5.24170399e-02\n",
      " -3.13678272e-02  1.77250057e-03  6.08697571e-02 -1.17368521e-02\n",
      " -4.43782099e-02  2.82229595e-02  6.04481995e-02 -1.47624435e-02\n",
      " -1.95627585e-02  2.19530538e-02  1.21970661e-02  5.82988188e-02\n",
      "  3.27155218e-02  1.99913308e-02  2.94112265e-02  3.14487307e-03\n",
      " -7.87445977e-02  2.12331135e-02  2.30186358e-02  1.12013649e-02\n",
      " -5.38815632e-02 -5.43773593e-03  7.51961581e-03 -1.66314822e-02\n",
      "  5.14522493e-02  2.07647495e-02  1.30601106e-02 -2.21293867e-02\n",
      " -6.76596761e-02  8.87383893e-02 -3.31702791e-02  6.75890669e-02\n",
      "  7.51317386e-03 -1.14760613e-02  2.58407947e-02  1.88138094e-02\n",
      "  2.15860400e-02 -5.00673465e-02  1.97226051e-02 -2.34714486e-02\n",
      "  1.58999413e-02 -1.78241171e-02 -3.36749852e-02 -3.12137641e-02\n",
      " -6.87364340e-02 -5.53952381e-02 -1.33226700e-02  9.07556266e-02\n",
      " -1.39441136e-02 -2.38397322e-03  1.20531069e-02  3.69711258e-02\n",
      "  5.66230109e-03 -5.47519289e-02  2.91745979e-02  2.53400095e-02\n",
      " -3.42108682e-02 -3.82893644e-02 -1.23289172e-02  5.43904603e-02\n",
      "  2.77680997e-02 -5.43448702e-02  6.56240061e-02  1.07708545e-02\n",
      "  4.95144650e-02  3.20130177e-02 -1.87052600e-02  2.38127019e-02\n",
      " -2.82576634e-03  4.89532314e-02 -4.05044295e-02  4.15044129e-02\n",
      "  2.21170112e-02  4.02254947e-02  6.62778853e-04 -2.66013294e-02\n",
      "  2.97558811e-02  1.16654141e-02 -2.73537319e-02 -2.47930996e-02\n",
      " -4.24888209e-02 -2.28427947e-02 -7.75750959e-03  1.06089916e-02\n",
      " -1.61925238e-02 -2.98666731e-02 -2.19001924e-03 -9.11666639e-03\n",
      " -4.31174152e-02  4.81004640e-02  2.91737821e-02  3.65316458e-02\n",
      "  8.62704683e-03  2.81584635e-02 -3.66027243e-02 -1.58778410e-02\n",
      "  2.89103016e-02  1.10300668e-02  2.22038245e-03  2.71217041e-02\n",
      "  5.57491183e-02  2.12214179e-02  1.79517362e-02 -5.60603337e-03\n",
      "  3.21495049e-02  3.18589434e-03 -1.22957472e-02 -2.84749955e-01\n",
      " -6.96145045e-03 -2.61184815e-02  1.19154025e-02  7.83517435e-02\n",
      "  6.87913150e-02 -4.61654291e-02  1.63812917e-02 -6.34703785e-02\n",
      "  1.68992244e-02  8.20981488e-02  4.63103019e-02 -4.62294742e-02\n",
      " -9.87647027e-02 -1.45330103e-02 -3.78732197e-02 -2.39125770e-02\n",
      " -9.94098336e-02 -1.14198081e-01 -4.03824039e-02 -3.15290131e-02\n",
      "  3.33460122e-02  1.66276377e-02 -3.46263312e-02  1.33976853e-02\n",
      "  3.79101671e-02  1.09187387e-01 -1.33951202e-01  2.23395936e-02\n",
      " -3.88490856e-02  5.41489050e-02  2.91531514e-02  3.00436560e-02\n",
      "  4.93388437e-02  1.28944237e-02 -3.49143520e-02 -9.56929550e-02\n",
      "  1.14745158e-03 -6.86828569e-02 -2.28722747e-02  1.17002465e-02\n",
      "  4.12341580e-02  4.42186417e-03  1.67704951e-02 -1.81469377e-02\n",
      " -3.65905128e-02 -5.85417729e-03  1.73980426e-02 -5.57877347e-02\n",
      " -1.85407382e-02 -9.39065963e-03  2.35821698e-02 -1.15334205e-02\n",
      " -2.20771151e-04  5.68484478e-02 -4.10349183e-02 -4.15649405e-03\n",
      " -1.87084284e-02 -5.27307913e-02 -6.82499185e-02  7.80460937e-03\n",
      " -1.20389266e-02  6.11249954e-02 -5.66756427e-02  8.04512482e-03\n",
      " -6.34470135e-02  2.60883830e-02 -6.24444075e-02  3.57052148e-03\n",
      " -1.47630377e-02 -4.05139960e-02  1.28105447e-01 -4.62464467e-02\n",
      " -4.68894504e-02  6.98431581e-02 -5.86285442e-02  2.61230581e-03\n",
      " -3.69492755e-03  1.88866220e-02 -2.72555364e-04  4.49167565e-02\n",
      " -7.01051578e-02  3.32547836e-02  4.16218955e-03  3.83976102e-02\n",
      " -6.02914654e-02 -8.84654187e-03  2.64486130e-02 -4.65537235e-03\n",
      "  8.31009373e-02 -2.78585665e-02 -5.86349629e-02  3.13871838e-02\n",
      " -5.05417697e-02 -2.45572664e-02  3.40867229e-02 -2.50407279e-01\n",
      "  3.68975773e-02  8.77449755e-03  2.07269820e-03 -5.87575324e-02\n",
      " -2.54632197e-02  3.68071347e-02  2.30085086e-02 -4.83227596e-02\n",
      "  4.44146916e-02 -1.84751227e-02  7.80806988e-02  4.26850803e-02\n",
      " -3.94337140e-02  4.10194993e-02  1.79833155e-02  3.49319316e-02\n",
      " -1.23652682e-01  1.93216633e-02 -3.15041877e-02  4.60592583e-02\n",
      " -1.27172284e-02  1.19911604e-01 -4.23560441e-02  2.49037482e-02\n",
      " -1.79259088e-02  4.25228942e-03  3.91551368e-02  2.21729055e-02\n",
      " -6.52894937e-03  3.54727358e-02  3.06253731e-02  4.10746150e-02\n",
      " -3.20202261e-02 -3.74302678e-02  5.52407466e-02  1.63820554e-02\n",
      "  6.98071793e-02  1.48774618e-02 -1.70219578e-02  7.49510713e-04\n",
      " -1.79578345e-02  5.51796146e-02 -2.13814750e-02  1.10728420e-01\n",
      " -6.24743011e-03  9.50526074e-03 -1.66353926e-01 -6.64762631e-02\n",
      "  2.18099263e-02 -8.30004811e-02  2.84278044e-03  1.75463762e-02\n",
      " -2.55270451e-02 -2.06205770e-02  4.00289223e-02  8.10526609e-02\n",
      " -1.77538767e-02  3.09484396e-02 -2.65116263e-02 -8.81683454e-03\n",
      " -4.80301259e-03  8.44882801e-02  5.55765145e-02 -1.53005682e-02]\n",
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "import  numpy as np\n",
    "print(np.array(Huggingface_embeddings.embed_query(final_documents[0].page_content)))\n",
    "print(np.array(Huggingface_embeddings.embed_query(final_documents[0].page_content)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63e26d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=FAISS.from_documents(final_documents[:120],Huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0136b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAT3003-Probability, Statictics and Reliability CW\n",
      "Reliability Engineering Concepts\n",
      "1. Reliability R(t)\n",
      "Reliability is a measure of the ability of a system or component to perform its required functions under\n",
      "stated conditions for a specified period of time. Mathematically, reliability is defined as the probability that\n",
      "a system or component does not fail within a given time t.\n",
      "The reliability function is expressed as:\n",
      "R(t) = P(T > t)\n",
      "where T is the random variable representing the time to failure.\n",
      "2. Cumulative Failure Function F(t)\n",
      "The cumulative failure function, also known as the cumulative distribution function (CDF) of the time to\n",
      "failure, represents the probability that the system or component will fail by a certain time t. It is the\n",
      "complement of the reliability function.\n",
      "The cumulative failure function is expressed as:\n",
      "F(t) = P(T ≤ t) = 1 − R(t)\n",
      "3. Failure Density Function f(t)\n"
     ]
    }
   ],
   "source": [
    "query=\"What is reliability?\"\n",
    "relevant_docments=vectorstore.similarity_search(query)\n",
    "\n",
    "print(relevant_docments[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3040854f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['FAISS', 'HuggingFaceBgeEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001A0ED636A50> search_kwargs={'k': 3}\n"
     ]
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bcdab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the token with error checking\n",
    "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89135f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b8a776dc924d41bcc43a11fcaf1900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ppriy\\.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf101a66c5d44700b1c3670ebfbbffa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77011f52ed4d4d1a827798d4ca150974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90239554a0d64af787c258dc1902fab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac60229f84b7402985c685c78bd09947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/863M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e9be38d66c44f889f1a08da299cd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`temperature` (=0) has to be a strictly positive float, otherwise your next token scores will be invalid.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 12\u001b[0m\n\u001b[0;32m      5\u001b[0m hf \u001b[38;5;241m=\u001b[39m HuggingFacePipeline\u001b[38;5;241m.\u001b[39mfrom_model_id(\n\u001b[0;32m      6\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/DialoGPT-medium\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Open access model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     pipeline_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m300\u001b[39m}\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m llm \u001b[38;5;241m=\u001b[39m hf \n\u001b[1;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(query)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:390\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    387\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    388\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    391\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    392\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    393\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    394\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    395\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    396\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    397\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    398\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    399\u001b[0m         )\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    402\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:789\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    787\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    788\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 789\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1000\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    987\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    988\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    998\u001b[0m         )\n\u001b[0;32m    999\u001b[0m     ]\n\u001b[1;32m-> 1000\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m   1001\u001b[0m         prompts,\n\u001b[0;32m   1002\u001b[0m         stop,\n\u001b[0;32m   1003\u001b[0m         run_managers,\n\u001b[0;32m   1004\u001b[0m         new_arg_supported\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(new_arg_supported),\n\u001b[0;32m   1005\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1006\u001b[0m     )\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1008\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1009\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m   1010\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[0;32m   1018\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:815\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    806\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    812\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 815\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    816\u001b[0m                 prompts,\n\u001b[0;32m    817\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    818\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    819\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    820\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    821\u001b[0m             )\n\u001b[0;32m    822\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    823\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    824\u001b[0m         )\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\huggingface_pipeline.py:285\u001b[0m, in \u001b[0;36mHuggingFacePipeline._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m batch_prompts \u001b[38;5;241m=\u001b[39m prompts[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[0;32m    286\u001b[0m     batch_prompts,\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpipeline_kwargs,\n\u001b[0;32m    288\u001b[0m )\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:332\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1448\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1445\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1446\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1447\u001b[0m     )\n\u001b[1;32m-> 1448\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(final_iterator)\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:126\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m    127\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:127\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m    126\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m--> 127\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1374\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1373\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1374\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1375\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:432\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[0;32m    430\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m--> 432\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[0;32m    435\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:2423\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2412\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2413\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are calling .generate() with the `input_ids` being on a device type different\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2414\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than your model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms device. `input_ids` is on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, whereas the model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2419\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   2420\u001b[0m     )\n\u001b[0;32m   2422\u001b[0m \u001b[38;5;66;03m# 9. prepare logits processors and stopping criteria\u001b[39;00m\n\u001b[1;32m-> 2423\u001b[0m prepared_logits_processor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_processor(\n\u001b[0;32m   2424\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2425\u001b[0m     input_ids_seq_length\u001b[38;5;241m=\u001b[39minput_ids_length,\n\u001b[0;32m   2426\u001b[0m     encoder_input_ids\u001b[38;5;241m=\u001b[39minputs_tensor,\n\u001b[0;32m   2427\u001b[0m     prefix_allowed_tokens_fn\u001b[38;5;241m=\u001b[39mprefix_allowed_tokens_fn,\n\u001b[0;32m   2428\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   2429\u001b[0m     device\u001b[38;5;241m=\u001b[39minputs_tensor\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m   2430\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[0;32m   2431\u001b[0m     negative_prompt_ids\u001b[38;5;241m=\u001b[39mnegative_prompt_ids,\n\u001b[0;32m   2432\u001b[0m     negative_prompt_attention_mask\u001b[38;5;241m=\u001b[39mnegative_prompt_attention_mask,\n\u001b[0;32m   2433\u001b[0m )\n\u001b[0;32m   2434\u001b[0m prepared_stopping_criteria \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_stopping_criteria(\n\u001b[0;32m   2435\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config, stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   2436\u001b[0m )\n\u001b[0;32m   2438\u001b[0m \u001b[38;5;66;03m# Set model_kwargs `use_cache` so we can use it later in forward runs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1268\u001b[0m, in \u001b[0;36mGenerationMixin._get_logits_processor\u001b[1;34m(self, generation_config, input_ids_seq_length, encoder_input_ids, prefix_allowed_tokens_fn, logits_processor, device, model_kwargs, negative_prompt_ids, negative_prompt_attention_mask)\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;66;03m# the following idea is largely copied from this PR: https://github.com/huggingface/transformers/pull/5420/files\u001b[39;00m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;66;03m# all samplers can be found in `generation_utils_samplers.py`\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mtemperature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mtemperature \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m-> 1268\u001b[0m     processors\u001b[38;5;241m.\u001b[39mappend(TemperatureLogitsWarper(generation_config\u001b[38;5;241m.\u001b[39mtemperature))\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mtop_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mtop_k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m     processors\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m   1271\u001b[0m         TopKLogitsWarper(top_k\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mtop_k, min_tokens_to_keep\u001b[38;5;241m=\u001b[39mmin_tokens_to_keep)\n\u001b[0;32m   1272\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ppriy\\anaconda3\\Lib\\site-packages\\transformers\\generation\\logits_process.py:287\u001b[0m, in \u001b[0;36mTemperatureLogitsWarper.__init__\u001b[1;34m(self, temperature)\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(temperature, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m temperature \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    286\u001b[0m         except_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m If you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre looking for greedy decoding strategies, set `do_sample=False`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(except_msg)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature \u001b[38;5;241m=\u001b[39m temperature\n",
      "\u001b[1;31mValueError\u001b[0m: `temperature` (=0) has to be a strictly positive float, otherwise your next token scores will be invalid."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9caa7659fc44e986d3eafd22d834c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/863M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Hugging Face models can be run locally through the HuggingFacePipeline class.\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# Use an unrestricted model\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"microsoft/DialoGPT-medium\",  # Open access model\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"temperature\": 0, \"max_new_tokens\": 300}\n",
    ")\n",
    "\n",
    "llm = hf \n",
    "response = llm.invoke(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
